{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2333,
     "status": "ok",
     "timestamp": 1559220594355,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "rQW1zvOJ-Xw1",
    "outputId": "52bd7669-d711-49d4-9a7b-63b45da17726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ex-yVMfI-Xw6"
   },
   "outputs": [],
   "source": [
    "\n",
    "filename = '/content/drive/My Drive/11-0.txt'\n",
    "#filename = 'C:/Users/sattar/Desktop/Second Semester/machine learning/5-8/11-0/11-0.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "#words = text.split()\n",
    "#print(words[:100])\n",
    "#len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1559220598253,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "tNMgzyK3-Xw9",
    "outputId": "cb69577c-c49e-46bc-d2ad-f3838fa859e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144330\n"
     ]
    }
   ],
   "source": [
    "z=text.split(\"CHAPTER I.\")\n",
    "#print(len(z[0]))\n",
    "#print(len(z[1]))\n",
    "#print()\n",
    "z=z[1]\n",
    "#print(len(z))\n",
    "z=z.split(\"THE END\")\n",
    "#print(len(z[0]))\n",
    "#print(len(z[1]))\n",
    "#print()\n",
    "z=z[0]\n",
    "\n",
    "print(len(z))\n",
    "fulltext=z;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1559220599704,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "Esxr9SOg-XxD",
    "outputId": "4443ea9f-137a-4895-fae1-fd68e0c3e37c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split in chapter in variable of chapters with length of 12\n",
    "x=fulltext.split(\"CHAPTER \")\n",
    "len(x[0])\n",
    "chapters=x;\n",
    "len(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zeke4CWL-XxI"
   },
   "outputs": [],
   "source": [
    "#find lines of Alice word\n",
    "lines=fulltext.split(\".\")\n",
    "#print(len(lines))\n",
    "#len(lines[900])\n",
    "AliceLines=[];\n",
    "for i in range(len(lines)):\n",
    "    #print(i)\n",
    "    if(lines[i].find(\"Alice\")!=-1):\n",
    "        AliceLines.append(lines[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1559220603048,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "-DvEb1UIFjvm",
    "outputId": "74a3f00f-5764-40a4-b2cf-9f7d3cbf5b68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AliceLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fbRqbHL-XxL"
   },
   "outputs": [],
   "source": [
    "#make a text with Alice line as one string and variable is Alice\n",
    "Alice=\"\"\n",
    "for i in range(len(AliceLines)):\n",
    "    Alice=Alice+\".\\n\"+AliceLines[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1559220605870,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "m4W_7hkw-XxN",
    "outputId": "75ba16ba-97ac-43a4-851d-abf68a89fb78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72553"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31IvOg66Gj_C"
   },
   "source": [
    "We have 3 important data\n",
    "1)**chapters** = include text of every chapter\n",
    "2)**Alice** =inluce Alice lines as a string\n",
    "3)**fulltext** which include all 12 chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1559220609754,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "zAQ8AIKB-XxR",
    "outputId": "f3b88d06-1374-4b05-8c16-6280a0bcc997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\t\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1241,
     "status": "ok",
     "timestamp": 1559220612114,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "C30qPSr0-XxV",
    "outputId": "e25e09a6-9dd2-4f0f-8bff-6e441866165e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "'''# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text'''\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# convert to lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "'''# load the document\n",
    "filename = 'C:/Users/sattar/Desktop/Second Semester/machine learning/5-8/11-0/11-0.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)\n",
    "len(tokens)'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNexvW4H-XxY"
   },
   "outputs": [],
   "source": [
    "len(chapters[0])\n",
    "clch4=clean_doc(chapters[4]);\n",
    "clch=[]\n",
    "for i in range(12):\n",
    "  clch.append(clean_doc(chapters[i]))\n",
    "#clch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1249,
     "status": "ok",
     "timestamp": 1559220620882,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "SntYZt7A-Xxb",
    "outputId": "6a00b7b9-17fc-4152-ab9e-e2d115bd0fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883\n",
      "883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clch[i] is a  cleaned data list for chapter i\n",
    "print(len(clch4))\n",
    "print(len(clch[4]))\n",
    "\n",
    "type(clch[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1559220642967,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "vOr2Nyl6SjiE",
    "outputId": "fa25557c-f622-435b-b48e-7c6fbfe04b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "12\n",
      "448\n",
      "922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vc is array of all word i  every chapter with frequency\n",
    "\n",
    "vc=[];\n",
    "print(type(vc))\n",
    "\n",
    "for i in range(12):\n",
    "  vocab = Counter(clch[i])\n",
    "  #print(len(vocab))\n",
    "  vv=vocab.most_common(len(vocab))\n",
    "  vc.append(vv)\n",
    "\n",
    "\n",
    "type(vv)\n",
    "print(len(vc))\n",
    "print(len(vc[8]))\n",
    "\n",
    "print(len(clch[8]))\n",
    "\n",
    "#vc[8]\n",
    "#vc[10][0][1]=float(vc[10][0][1])\n",
    "\n",
    "freq=[]\n",
    "for i in range(12):\n",
    "  freq.append([])\n",
    "  \n",
    "type(freq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1559220645484,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "JlvlVUnCep6z",
    "outputId": "361741e2-8846-4381-9751-7c3ec2a3cada"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alice'"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chapter number(in len) name\n",
    "vc[0][0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPNPpaPcdHSl"
   },
   "source": [
    "now we have to important matrices\n",
    "**vc** includ all words and couts\n",
    "**freq** a list of 12 list\n",
    "so we must calculate each word in which chapter exust **n/12**\n",
    "\n",
    "lst=[]\n",
    "for j in range(len(vc[i])):\n",
    "  rinvc=0\n",
    "  for k in range(12):\n",
    "    if vc[i][j][0] in clch[k]:\n",
    "      rinvc=rinvc+1  \n",
    "  lst.append(rinvc)  \n",
    "freq.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J1N9YCMpeTn8"
   },
   "outputs": [],
   "source": [
    "type(freq[0])\n",
    "for i in range(12):\n",
    "  lst=[]\n",
    "  for j in range(len(vc[i])):\n",
    "    rinvc=0\n",
    "    for k in range(12):\n",
    "      if vc[i][j][0] in clch[k]:\n",
    "        rinvc=rinvc+1\n",
    "    lst.append(rinvc)\n",
    "  freq[i].append(lst)\n",
    "  \n",
    " \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1559220651965,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "WwZOhG7Ej4fX",
    "outputId": "3dcfd235-03cf-483d-b347-778e2931a09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n",
      "484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('alice', 24),\n",
       " ('little', 16),\n",
       " ('mouse', 13),\n",
       " ('said', 12),\n",
       " ('go', 10),\n",
       " ('like', 9),\n",
       " ('must', 9),\n",
       " ('thought', 9),\n",
       " ('went', 9),\n",
       " ('pool', 8),\n",
       " ('cried', 8),\n",
       " ('way', 8),\n",
       " ('feet', 7),\n",
       " ('could', 7),\n",
       " ('one', 7),\n",
       " ('tears', 6),\n",
       " ('see', 6),\n",
       " ('oh', 6),\n",
       " ('dear', 6),\n",
       " ('began', 6),\n",
       " ('things', 6),\n",
       " ('know', 6),\n",
       " ('quite', 5),\n",
       " ('poor', 5),\n",
       " ('sure', 5),\n",
       " ('shall', 5),\n",
       " ('great', 5),\n",
       " ('time', 5),\n",
       " ('come', 5),\n",
       " ('swam', 5),\n",
       " ('cats', 5),\n",
       " ('let', 4),\n",
       " ('high', 4),\n",
       " ('four', 4),\n",
       " ('white', 4),\n",
       " ('gloves', 4),\n",
       " ('fan', 4),\n",
       " ('voice', 4),\n",
       " ('think', 4),\n",
       " ('mabel', 4),\n",
       " ('never', 4),\n",
       " ('first', 4),\n",
       " ('back', 4),\n",
       " ('talk', 4),\n",
       " ('moment', 3),\n",
       " ('ever', 3),\n",
       " ('looked', 3),\n",
       " ('seemed', 3),\n",
       " ('put', 3),\n",
       " ('would', 3),\n",
       " ('look', 3),\n",
       " ('right', 3),\n",
       " ('near', 3),\n",
       " ('hall', 3),\n",
       " ('golden', 3),\n",
       " ('door', 3),\n",
       " ('get', 3),\n",
       " ('say', 3),\n",
       " ('tell', 3),\n",
       " ('half', 3),\n",
       " ('heard', 3),\n",
       " ('hastily', 3),\n",
       " ('eyes', 3),\n",
       " ('rabbit', 3),\n",
       " ('kid', 3),\n",
       " ('came', 3),\n",
       " ('low', 3),\n",
       " ('away', 3),\n",
       " ('queer', 3),\n",
       " ('everything', 3),\n",
       " ('changed', 3),\n",
       " ('long', 3),\n",
       " ('try', 3),\n",
       " ('times', 3),\n",
       " ('however', 3),\n",
       " ('table', 3),\n",
       " ('capital', 3),\n",
       " ('made', 3),\n",
       " ('sudden', 3),\n",
       " ('wish', 3),\n",
       " ('small', 3),\n",
       " ('soon', 3),\n",
       " ('find', 3),\n",
       " ('thing', 3),\n",
       " ('beg', 3),\n",
       " ('much', 2),\n",
       " ('surprised', 2),\n",
       " ('forgot', 2),\n",
       " ('speak', 2),\n",
       " ('good', 2),\n",
       " ('english', 2),\n",
       " ('almost', 2),\n",
       " ('getting', 2),\n",
       " ('far', 2),\n",
       " ('wonder', 2),\n",
       " ('deal', 2),\n",
       " ('manage', 2),\n",
       " ('perhaps', 2),\n",
       " ('pair', 2),\n",
       " ('every', 2),\n",
       " ('foot', 2),\n",
       " ('nine', 2),\n",
       " ('took', 2),\n",
       " ('key', 2),\n",
       " ('garden', 2),\n",
       " ('lying', 2),\n",
       " ('large', 2),\n",
       " ('round', 2),\n",
       " ('hurry', 2),\n",
       " ('duchess', 2),\n",
       " ('kept', 2),\n",
       " ('felt', 2),\n",
       " ('dropped', 2),\n",
       " ('hard', 2),\n",
       " ('talking', 2),\n",
       " ('got', 2),\n",
       " ('remember', 2),\n",
       " ('next', 2),\n",
       " ('children', 2),\n",
       " ('hair', 2),\n",
       " ('ringlets', 2),\n",
       " ('sorts', 2),\n",
       " ('andoh', 2),\n",
       " ('used', 2),\n",
       " ('rate', 2),\n",
       " ('paris', 2),\n",
       " ('certain', 2),\n",
       " ('doth', 2),\n",
       " ('hands', 2),\n",
       " ('saying', 2),\n",
       " ('lessons', 2),\n",
       " ('words', 2),\n",
       " ('tail', 2),\n",
       " ('house', 2),\n",
       " ('stay', 2),\n",
       " ('use', 2),\n",
       " ('heads', 2),\n",
       " ('tired', 2),\n",
       " ('done', 2),\n",
       " ('found', 2),\n",
       " ('shrinking', 2),\n",
       " ('change', 2),\n",
       " ('slipped', 2),\n",
       " ('water', 2),\n",
       " ('fallen', 2),\n",
       " ('sea', 2),\n",
       " ('remembered', 2),\n",
       " ('swimming', 2),\n",
       " ('rather', 2),\n",
       " ('understand', 2),\n",
       " ('french', 2),\n",
       " ('history', 2),\n",
       " ('afraid', 2),\n",
       " ('tone', 2),\n",
       " ('show', 2),\n",
       " ('nice', 2),\n",
       " ('offended', 2),\n",
       " ('trembling', 2),\n",
       " ('subject', 2),\n",
       " ('says', 2),\n",
       " ('shore', 2),\n",
       " ('ii', 1),\n",
       " ('opening', 1),\n",
       " ('largest', 1),\n",
       " ('telescope', 1),\n",
       " ('goodbye', 1),\n",
       " ('sight', 1),\n",
       " ('shoes', 1),\n",
       " ('stockings', 1),\n",
       " ('dears', 1),\n",
       " ('able', 1),\n",
       " ('trouble', 1),\n",
       " ('best', 1),\n",
       " ('canbut', 1),\n",
       " ('kind', 1),\n",
       " ('walk', 1),\n",
       " ('want', 1),\n",
       " ('give', 1),\n",
       " ('new', 1),\n",
       " ('boots', 1),\n",
       " ('planning', 1),\n",
       " ('funny', 1),\n",
       " ('seem', 1),\n",
       " ('sending', 1),\n",
       " ('presents', 1),\n",
       " ('odd', 1),\n",
       " ('directions', 1),\n",
       " ('esq', 1),\n",
       " ('hearthrug', 1),\n",
       " ('fender', 1),\n",
       " ('love', 1),\n",
       " ('nonsense', 1),\n",
       " ('head', 1),\n",
       " ('struck', 1),\n",
       " ('roof', 1),\n",
       " ('fact', 1),\n",
       " ('hurried', 1),\n",
       " ('side', 1),\n",
       " ('eye', 1),\n",
       " ('hopeless', 1),\n",
       " ('sat', 1),\n",
       " ('cry', 1),\n",
       " ('ought', 1),\n",
       " ('ashamed', 1),\n",
       " ('girl', 1),\n",
       " ('might', 1),\n",
       " ('well', 1),\n",
       " ('crying', 1),\n",
       " ('stop', 1),\n",
       " ('shedding', 1),\n",
       " ('gallons', 1),\n",
       " ('inches', 1),\n",
       " ('deep', 1),\n",
       " ('reaching', 1),\n",
       " ('pattering', 1),\n",
       " ('distance', 1),\n",
       " ('dried', 1),\n",
       " ('coming', 1),\n",
       " ('returning', 1),\n",
       " ('splendidly', 1),\n",
       " ('dressed', 1),\n",
       " ('hand', 1),\n",
       " ('trotting', 1),\n",
       " ('along', 1),\n",
       " ('muttering', 1),\n",
       " ('savage', 1),\n",
       " ('desperate', 1),\n",
       " ('ready', 1),\n",
       " ('ask', 1),\n",
       " ('help', 1),\n",
       " ('timid', 1),\n",
       " ('please', 1),\n",
       " ('started', 1),\n",
       " ('violently', 1),\n",
       " ('skurried', 1),\n",
       " ('darkness', 1),\n",
       " ('hot', 1),\n",
       " ('fanning', 1),\n",
       " ('today', 1),\n",
       " ('yesterday', 1),\n",
       " ('usual', 1),\n",
       " ('night', 1),\n",
       " ('morning', 1),\n",
       " ('feeling', 1),\n",
       " ('different', 1),\n",
       " ('question', 1),\n",
       " ('world', 1),\n",
       " ('ah', 1),\n",
       " ('thinking', 1),\n",
       " ('knew', 1),\n",
       " ('age', 1),\n",
       " ('goes', 1),\n",
       " ('mine', 1),\n",
       " ('knows', 1),\n",
       " ('besides', 1),\n",
       " ('puzzling', 1),\n",
       " ('five', 1),\n",
       " ('twelve', 1),\n",
       " ('six', 1),\n",
       " ('thirteen', 1),\n",
       " ('seven', 1),\n",
       " ('isoh', 1),\n",
       " ('twenty', 1),\n",
       " ('multiplication', 1),\n",
       " ('signify', 1),\n",
       " ('geography', 1),\n",
       " ('london', 1),\n",
       " ('rome', 1),\n",
       " ('romeno', 1),\n",
       " ('wrong', 1),\n",
       " ('crossed', 1),\n",
       " ('lap', 1),\n",
       " ('repeat', 1),\n",
       " ('sounded', 1),\n",
       " ('hoarse', 1),\n",
       " ('strange', 1),\n",
       " ('crocodile', 1),\n",
       " ('improve', 1),\n",
       " ('shining', 1),\n",
       " ('pour', 1),\n",
       " ('waters', 1),\n",
       " ('nile', 1),\n",
       " ('scale', 1),\n",
       " ('cheerfully', 1),\n",
       " ('seems', 1),\n",
       " ('grin', 1),\n",
       " ('neatly', 1),\n",
       " ('spread', 1),\n",
       " ('claws', 1),\n",
       " ('welcome', 1),\n",
       " ('fishes', 1),\n",
       " ('gently', 1),\n",
       " ('smiling', 1),\n",
       " ('filled', 1),\n",
       " ('live', 1),\n",
       " ('poky', 1),\n",
       " ('toys', 1),\n",
       " ('play', 1),\n",
       " ('many', 1),\n",
       " ('learn', 1),\n",
       " ('mind', 1),\n",
       " ('putting', 1),\n",
       " ('person', 1),\n",
       " ('till', 1),\n",
       " ('somebody', 1),\n",
       " ('burst', 1),\n",
       " ('alone', 1),\n",
       " ('growing', 1),\n",
       " ('measure', 1),\n",
       " ('nearly', 1),\n",
       " ('guess', 1),\n",
       " ('two', 1),\n",
       " ('going', 1),\n",
       " ('rapidly', 1),\n",
       " ('cause', 1),\n",
       " ('holding', 1),\n",
       " ('avoid', 1),\n",
       " ('altogether', 1),\n",
       " ('narrow', 1),\n",
       " ('frightened', 1),\n",
       " ('glad', 1),\n",
       " ('still', 1),\n",
       " ('existence', 1),\n",
       " ('ran', 1),\n",
       " ('speed', 1),\n",
       " ('alas', 1),\n",
       " ('shut', 1),\n",
       " ('glass', 1),\n",
       " ('worse', 1),\n",
       " ('child', 1),\n",
       " ('declare', 1),\n",
       " ('bad', 1),\n",
       " ('another', 1),\n",
       " ('splash', 1),\n",
       " ('chin', 1),\n",
       " ('salt', 1),\n",
       " ('idea', 1),\n",
       " ('somehow', 1),\n",
       " ('case', 1),\n",
       " ('seaside', 1),\n",
       " ('life', 1),\n",
       " ('general', 1),\n",
       " ('conclusion', 1),\n",
       " ('wherever', 1),\n",
       " ('coast', 1),\n",
       " ('number', 1),\n",
       " ('bathing', 1),\n",
       " ('machines', 1),\n",
       " ('digging', 1),\n",
       " ('sand', 1),\n",
       " ('wooden', 1),\n",
       " ('spades', 1),\n",
       " ('row', 1),\n",
       " ('lodging', 1),\n",
       " ('houses', 1),\n",
       " ('behind', 1),\n",
       " ('railway', 1),\n",
       " ('station', 1),\n",
       " ('wept', 1),\n",
       " ('trying', 1),\n",
       " ('punished', 1),\n",
       " ('suppose', 1),\n",
       " ('drowned', 1),\n",
       " ('something', 1),\n",
       " ('splashing', 1),\n",
       " ('nearer', 1),\n",
       " ('make', 1),\n",
       " ('walrus', 1),\n",
       " ('hippopotamus', 1),\n",
       " ('outoftheway', 1),\n",
       " ('likely', 1),\n",
       " ('harm', 1),\n",
       " ('speaking', 1),\n",
       " ('seen', 1),\n",
       " ('latin', 1),\n",
       " ('grammar', 1),\n",
       " ('mouseof', 1),\n",
       " ('mouseto', 1),\n",
       " ('mousea', 1),\n",
       " ('mouseo', 1),\n",
       " ('inquisitively', 1),\n",
       " ('wink', 1),\n",
       " ('nothing', 1),\n",
       " ('daresay', 1),\n",
       " ('william', 1),\n",
       " ('knowledge', 1),\n",
       " ('clear', 1),\n",
       " ('notion', 1),\n",
       " ('ago', 1),\n",
       " ('anything', 1),\n",
       " ('happened', 1),\n",
       " ('est', 1),\n",
       " ('sentence', 1),\n",
       " ('lessonbook', 1),\n",
       " ('gave', 1),\n",
       " ('leap', 1),\n",
       " ('quiver', 1),\n",
       " ('fright', 1),\n",
       " ('hurt', 1),\n",
       " ('feelings', 1),\n",
       " ('shrill', 1),\n",
       " ('passionate', 1),\n",
       " ('soothing', 1),\n",
       " ('angry', 1),\n",
       " ('yet', 1),\n",
       " ('cat', 1),\n",
       " ('dinah', 1),\n",
       " ('take', 1),\n",
       " ('fancy', 1),\n",
       " ('quiet', 1),\n",
       " ('lazily', 1),\n",
       " ('sits', 1),\n",
       " ('purring', 1),\n",
       " ('nicely', 1),\n",
       " ('fire', 1),\n",
       " ('licking', 1),\n",
       " ('paws', 1),\n",
       " ('washing', 1),\n",
       " ('faceand', 1),\n",
       " ('soft', 1),\n",
       " ('nurseand', 1),\n",
       " ('catching', 1),\n",
       " ('miceoh', 1),\n",
       " ('bristling', 1),\n",
       " ('really', 1),\n",
       " ('end', 1),\n",
       " ('family', 1),\n",
       " ('always', 1),\n",
       " ('hated', 1),\n",
       " ('nasty', 1),\n",
       " ('vulgar', 1),\n",
       " ('hear', 1),\n",
       " ('name', 1),\n",
       " ('conversation', 1),\n",
       " ('youare', 1),\n",
       " ('fondofof', 1),\n",
       " ('answer', 1),\n",
       " ('eagerly', 1),\n",
       " ('dog', 1),\n",
       " ('brighteyed', 1),\n",
       " ('terrier', 1),\n",
       " ('curly', 1),\n",
       " ('brown', 1),\n",
       " ('fetch', 1),\n",
       " ('throw', 1),\n",
       " ('sit', 1),\n",
       " ('dinner', 1),\n",
       " ('thingsi', 1),\n",
       " ('themand', 1),\n",
       " ('belongs', 1),\n",
       " ('farmer', 1),\n",
       " ('useful', 1),\n",
       " ('worth', 1),\n",
       " ('hundred', 1),\n",
       " ('pounds', 1),\n",
       " ('kills', 1),\n",
       " ('rats', 1),\n",
       " ('sorrowful', 1),\n",
       " ('making', 1),\n",
       " ('commotion', 1),\n",
       " ('called', 1),\n",
       " ('softly', 1),\n",
       " ('dogs', 1),\n",
       " ('either', 1),\n",
       " ('turned', 1),\n",
       " ('slowly', 1),\n",
       " ('face', 1),\n",
       " ('pale', 1),\n",
       " ('passion', 1),\n",
       " ('us', 1),\n",
       " ('hate', 1),\n",
       " ('crowded', 1),\n",
       " ('birds', 1),\n",
       " ('animals', 1),\n",
       " ('duck', 1),\n",
       " ('dodo', 1),\n",
       " ('lory', 1),\n",
       " ('eaglet', 1),\n",
       " ('several', 1),\n",
       " ('curious', 1),\n",
       " ('creatures', 1),\n",
       " ('led', 1),\n",
       " ('whole', 1),\n",
       " ('party', 1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(freq[1][0]))\n",
    "print(len(vc[1]))\n",
    "type(freq[0])\n",
    "freq[1]\n",
    "#vc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1205,
     "status": "ok",
     "timestamp": 1559221020422,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "ihlD6wu5omZp",
    "outputId": "a69fbf80-a992-4997-fafd-53aaa12d3ed3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>king</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jury</td>\n",
       "      <td>0.000464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sister</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>jurymen</td>\n",
       "      <td>0.000370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dream</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>jurybox</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>verses</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>slates</td>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>paper</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word        tf\n",
       "2      king  0.000900\n",
       "5      jury  0.000464\n",
       "16   sister  0.000400\n",
       "19  jurymen  0.000370\n",
       "17    dream  0.000333\n",
       "29  jurybox  0.000277\n",
       "53   verses  0.000277\n",
       "23   slates  0.000267\n",
       "6     queen  0.000232\n",
       "48    paper  0.000200"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd \n",
    "chnum=11\n",
    "word=[]\n",
    "wordcount=[]\n",
    "wordfreq=[]\n",
    "tf=[]\n",
    "for i in range(len(vc[chnum])):\n",
    "  word.append(vc[chnum][i][0])\n",
    "  wordcount.append(vc[chnum][i][1])\n",
    "  wordfreq.append(freq[chnum][0][i])\n",
    "#print(len(word))\n",
    "#print(len(wordcount))\n",
    "#print(len(wordfreq))\n",
    "#vc[1][0:10][0]\n",
    "for i in range(len(word)):\n",
    "  result=math.log(12/wordfreq[i] ,10)\n",
    "  result=result*wordcount[i]/len(chapters[chnum])\n",
    "  tf.append(result)\n",
    "\n",
    "#word\n",
    "#tf\n",
    "\n",
    "data = {'word':word, 'tf':tf} \n",
    "  \n",
    "# Create DataFrame \n",
    "df = pd.DataFrame(data)\n",
    "sort_by_tf = df.sort_values('tf',ascending=False)\n",
    "sort_by_tf.head(10)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWOK0Qwz-Xxo"
   },
   "outputs": [],
   "source": [
    "chcount=[]\n",
    "for i in range(len(chapters)):\n",
    "    clch=clean_doc(chapters[i])\n",
    "    vocab = Counter(clch)\n",
    "    vc=vocab.most_common(11)\n",
    "    chcount.append(vc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1559219639887,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "ngw-56Wp-Xxr",
    "outputId": "dd9e3000-0f38-4684-9b26-0c04e9aa4c65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 59),\n",
       " ('alice', 50),\n",
       " ('hatter', 32),\n",
       " ('dormouse', 24),\n",
       " ('march', 21),\n",
       " ('hare', 21),\n",
       " ('time', 14),\n",
       " ('went', 10),\n",
       " ('little', 9),\n",
       " ('one', 8),\n",
       " ('thing', 8)]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chcount[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1559219643763,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "GluNQACW-Xx3",
    "outputId": "8b73f8e4-4211-4e95-d5c4-e2ba69e9d6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFXkNKPV-Xx8"
   },
   "outputs": [],
   "source": [
    "\n",
    "#text = word_tokenize(\"Here is a simple way of doing this\")\n",
    "tokens=clean_doc(fulltext)\n",
    "word_tag_fd = pos_tag(tokens, tagset='universal')\n",
    "\n",
    "\n",
    "verb=[v for i, v in enumerate(word_tag_fd) if v[1] == 'VERB'];\n",
    "verbs = Counter(verb)\n",
    "v=verbs.most_common(15)\n",
    "#verb=[wt[0] for (wt, _) in word_tag_fd.most_common() if wt[1] == 'VERB']\n",
    "\n",
    "#print(word_tag_fd)\n",
    "#wsj = nltk.corpus.treebank.tagged_words(tagset='universal')\n",
    "#word_tag_fd = nltk.FreqDist(wsj)\n",
    "#verb=[wt[0] for (wt, _) in word_tag_fd.most_common() if wt[1] == 'VERB']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1193,
     "status": "ok",
     "timestamp": 1559219649197,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "8DQWWJUF0nOn",
    "outputId": "b82f761a-2e0a-461e-a947-7d0123f1c352"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('went', 'VERB'), 83)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1397,
     "status": "ok",
     "timestamp": 1559219651470,
     "user": {
      "displayName": "Satar Shamsi",
      "photoUrl": "https://lh5.googleusercontent.com/--Elob3Ufmsw/AAAAAAAAAAI/AAAAAAAAAC0/aSJFzXffJVc/s64/photo.jpg",
      "userId": "07166502641627322948"
     },
     "user_tz": -180
    },
    "id": "S8Zptuvb-Xx_",
    "outputId": "47ffbee8-3f70-47ef-b041-27c2e1ae7260"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('said', 'VERB'), 457),\n",
       " (('went', 'VERB'), 83),\n",
       " (('could', 'VERB'), 77),\n",
       " (('would', 'VERB'), 77),\n",
       " (('began', 'VERB'), 58),\n",
       " (('got', 'VERB'), 45),\n",
       " (('thought', 'VERB'), 44),\n",
       " (('see', 'VERB'), 44),\n",
       " (('must', 'VERB'), 43),\n",
       " (('say', 'VERB'), 41),\n",
       " (('looked', 'VERB'), 40),\n",
       " (('came', 'VERB'), 40),\n",
       " (('go', 'VERB'), 40),\n",
       " (('know', 'VERB'), 38),\n",
       " (('think', 'VERB'), 33)]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v[:5][0][0])\n",
    "AV=[]\n",
    "for i in range(len(v)):\n",
    "    AV.append(v[i])\n",
    "AV"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ALICE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
